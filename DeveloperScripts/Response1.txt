Letter to the Editor:

Hello Daniel,
Many thanks for encouraging submission of the attached revised manuscript. My group worked to conscientiously address the reviewers' concerns and we believe the new experiments and supplementary data now included should fulfill their requests. Based on the past success of papers that present an approach along with usable software to accomplish it, we anticipate that the paper, if published, should be quite popular.

Sincerely,
Anne




Response to Reviewers' Comments, and guide to changes in the manuscript:  

Reviewer #1:

Summary
The authors describe a supervised machine learning approach to score phenotypes in high-throughput image-based screens. The approach requires human experts to provide examples of cells with and without phenotypes of interest. A machine learning algorithm is then trained to recognize the provided cells, and opinions from human experts are used iteratively to improve the classification accuracy.

Major Comments
COMMENT 1A: There is an extensive prior works in the area of automated high throughput cellular phenotype profiling using microscopy images. The statement that (past) "automated image analysis requires extensive customization to each phenotype ..." is not true. For example, unbiased sets of measurements that are not customized for any particular phenotypes were used in Boland, et al., Bioinformatics (2001), Tanaka, et. al., PLOS Biology (2005), and Loo, et al., Nature Methods (2007).

Response:
Agreed, our meaning was unclear in that sentence. We replaced the sentence with: "Automated image analysis coupled with machine learning can effectively score many phenotypes. However, significant effort may be required to find a sufficient number of positive example cells to train the machine learning algorithm, particularly when positive control samples are not available."


COMMENT 1B: Further more, the authors' claim that "none has been implemented in a system proven capable of scoring millions of cells for many diverse morphologies - especially for phenotypes that are complex or subtle, without known positive controls, or present in only a small percentage of the cells in a sample". However, it appear that, as many other prior works, the proposed method also requires positive samples provided from human experts. Thus, it is unclear what do the authors mean by "without known positive controls"?

Response:
The distinction between positive control samples and positive control cells was not clear in our original sentence. To clarify, we have replaced the sentence with: "These methods require providing examples of cells that do and do not display the morphology of interest (i.e., positive and negative cells). However, finding a sufficient number of positive cells can be prohibitively difficult when positive control samples are not available and when the phenotype is rare in the wild-type population."


COMMENT 1C: The idea of using human experts to guide classifier is neither novel nor interesting. The proposed approach of identifying positive control cells from "cells noticed by chance while browsing images from the screen, or outlier cells discovered by interactively exploring cell measurements" is highly biased, non-systematic, and, worst of all, probabilistic (i.e., may not be reproducible). Although the proposed method is good at "zooming" into the phenotypes selected by human experts, it is unclear how the proposed system may discover novel cells or phenotypes that are missed by the human experts.

Response:
The goal of our work is phenotype scoring, not discovery of novel phenotypes. The sentence mentioned above describes how a researcher might choose an interesting phenotype that they wish to pursue; historically, selecting phenotypes for study has indeed been highly biased (i.e., guided by the intuition of a biologist), particularly in classic screens that have been the basis of much of modern biology. Methods exist for automated phenotype discovery, but this is not addressed by our work. We have revised the sentence to clarify: "These can be cells from control samples if the screen has been designed to address a particular phenotype, or cells identified by chance if the screen's goal is to uncover previously uncharacterized phenotypes, as in many classic screens."


COMMENT 1D: This manuscript lacks methodological novelty or biological discovery to make this appropriate for Nature Methods.

Response:
Please see our response to Reviewer 3, comment 3H.

===========

Reviewer #2:

This ms reports a new software tool, Classifier, that the authors have implemented within their existing CellAnalyst package.  Classifier is aptly named-it is a tool that provides a nice, apparently easy-to-use interface for running sophisticated feature-based supervised learning.  

There are a few successes in this field, but none to my knowledge that are routinely applied, distributed and available.  The authors should be commended for their efforts and commitment to developing useful and available software.  To my knowledge, this is unique in the image classification domain, and an important contribution for the field.  

COMMENT 2A: However, does this very useful implementation constitute a significant methodological or conceptual advance?  This is a surprisingly light treatment of an important topic that the authors properly and critically introduce-the use of automatic, supervised learning methods in large-scale cell-based screens.  As such the authors don't reach the standard achieved by other published papers in this field- reports of confusion matrices (as opposed to the very simple right/wrong analysis provided here), discussion of different classifier implementations, examination of feature space explored by different phenotypes. 

Response:
The referee points out that richer would be helpful to evaluate the methods we have described. We excluded these from the original manuscript to maintain sharp focus and better fit the size limitations, but they are now included in the revised version:
(a) Confusion matrices (Supplementary Figure [[XXX - confusion matrix]]) support the validation results originally shown in Figure 3 ("Validation" column).  In addition to the existing statement that there were no false negatives and, in total, two false positives across the 720 samples scored by eye, we have added this sentence: "Agreement between humans was comparable to that between humans and automated scoring (Supplementary Figure [[XXX - confusion matrix]])."

(b) Discussion of different classifier implementations:
Evaluation of various classifier algorithms has indeed been heavily explored in others' work, both for cell screens in the Murphy lab, as well as for many other domains in the general machine learning community. One conclusion from this work is that the top classification methods (e.g., SVMs, neural networks, boosting variants) perform roughly similarly.  Contributing to the work comparing particular learning algorithms is not our goal; rather, we present a method for rapidly creating useful training sets for rare phenotypes, using an arbitrary machine learning algorithm. In fact, we believe that any of these methods would perform well within our framework - our initial testing indicated that SVMs performed similarly to GentleBoosting on regression stumps but we chose the latter because SVMs were less suitable for generating the database queries necessary for scoring entire screens. To clarify this, we have added a sentence: "Other machine learning methods are likely to be equally effective, based on their performance in previous work [[References 13-22]]."

(c) Examination of feature space explored by different phenotypes:
We have created a Supplementary Figure [[XXX]] to facilitate examination of the feature space explored by each phenotype.

Figure Legend: "Categories of features relevant to each phenotype. For each phenotype's 50-rule classifier, the chart shows the number of individual rules that reference a particular feature category (X-axis: texture, intensity, shape, other). "Texture", "Intensity", and "Shape" indicate all features of the particular type, regardless of color channel, and are subdivided by cellular compartment within which the feature was calculated: nucleus, entire cell, or cytoplasm (cell body excluding nucleus). Correlation between channels is included with the Intensity category. "Other" includes neighbor counts of cells and cell location within images."


COMMENT 2B: The authors' statement that "...automated scoring of unusual phenotypes in general can now be accomplished, if accurate cytoprofiles can be generated for the cell images" doesn't seem novel.  Admittedly, many other papers from Eils, Murphy, Ellenberg & Pepperkok, and many others referenced by the authors have focused on automatic identification of human-recognized phenotypes, but Murphy's group have already established good methods for  distinguishing phenotypes based on molecular identity that were not known to be distinguishable by humans-in short classifiers can find new phenotypes.  Against this background, the authors' contention of a significant conceptual advance seems dubious. 

Response:
We agree that the original sentence unintentionally implied that we applied machine learning to cell screening for the first time. We have adjusted the sentence to appropriately qualify the statement: "automated scoring of a wide variety of morphologies can now be accomplished quickly and easily, even when the phenotype is rare in the wild-type population and positive control samples are not available."


COMMENT 2C: The authors are first-class scientists, therefore it's worth posing a hypothetical question--  if I submitted a paper to Nature Methods reporting a new method based on an application of that method to a single dataset, would they consider that a reasonable demonstration of the general utility of the method?  This report describes the use of Classifier on one cell-based screen.  That implementation is certainly an achievement, but the authors (and editors of this journal, who have been guilty of this!) must consider the impact of reporting results based on a single run, in the absence of any significant finding derived from this analysis.  This does not diminish the work the authors have done, but recognizes the importance of delivering tested, generally useful methodology to the community.  The use of Classifier to define phenotypes across multiple screens, with multiple probes is therefore most important. Even surveying the types of assays most often done in HCS, and critically evaluating the phenotypes that are well detected and those that are less well-detected would be sufficient.  The authors must have access to this data.

Response:
The reviewer makes an excellent point - indeed, several other projects using Classifier have been successful, and we have now included analysis of two additional image sets in the revised manuscript. These are different screens, with different stains, in different cell lines, from different species, and by different investigators. One is described more fully in our response to Reviewer 3's comment 3G about overfitting and how to score replicates performed on different days (please see that response below). The second is described in the following new paragraph: "Lastly, we tested ClassifierÕs flexibility by applying it to another large-scale image set. In this experiment, NN genes were previously screened... [[Ray TODO: For new fly200 metaphase screen, produce an error rate vs rules plot (as you are doing for the Eggert/Castoreno data, possibly dividing up the training sets across the two slides), and make Fly200 forced choice tests for Anne & Chris to do. Anne TODO: describe the results, with reference to new supplementary figures Ray is making; this new paragraph goes just before the discussion]]."

We should also elaborate on the screens mentioned only briefly in the text: we have assisted in the completion of several large-scale screens that probe phenotypes distinct from each other and from those described in this manuscript. They were conducted by different investigators, in different cell types/organisms, using different image acquisition devices at different magnifications. These screens are, of course, intended for separate publication so we have not added detailed description to this manuscript but instead elaborate on a few of them here:

[[Gupta.tif to be pasted here (currently on imaging/analysis)]]
(a) Piyush Gupta and others in Eric Lander's lab have used our approach/software to complete an RNAi screen for regulators of the subtle cytoskeletal changes induced by a growth factor (HRG-b1) on T47D human breast cancer cells.

[[Mitchison.psd to be pasted here (currently on imaging/analysis)]]
(b) Taio Xie and others in Tim Mitchison's lab have used our approach/software to complete two genome-wide RNAi screens in HeLa cells related to spindle morphology during cell division, in enhancer/suppressor screens using two levels of a Kinesin-V inhibitor. The system was trained to identify monopolar mitotic cells.

[[Eggert.psd to be pasted here (currently on imaging/analysis)]]
(c) Adam Castoreno and others in Riki Eggert's lab have processed over 100,000 images using our approach/software for a chemical screen in Drosophila Kc167 cells to identify modulators of a binucleate phenotype induced by RNA interference against Rho. 


COMMENT 2D: Moreover, a few practical but very important considerations are missing:
COMMENT 2D.1: No report on the computational performance of Classifier is mentioned.  How long does training take?  How does this scale with number of images, channels, image size, etc.?  What kinds of hardware are used for this calculation.  The authors have a very impressive compute facility at their disposal-is this required? Perhaps the application is limited by I/O, given its emphasis on large numbers of images.  Reporting these limitations is absolutely essential. 

Response:
We have added the following to the methods: "Computing a classifier, given a training set, is fast (for example, 25 seconds to generate a 50-rule classifier for the 1711-example Crescent Nuclei training set on a 2.33 GHz Intel Core 2 Duo), and grows linearly with training set size and the number of features. Evaluation of the classifier to classify all 8 million cells in the database takes roughly 2 minutes, with the same orders of growth. This evaluation is primarily limited by disk transfer speed, as the full data set must be read in order to score every cell. Image processing times to identify and measure cells using CellProfiler are currently on the order of 10 seconds to several minutes per image, depending on the particular experimental and image analysis used (for example, roughly 2.5 minutes per three-channel 512 x 512 image on a [[Ray TODO: what speed computer here? It was node392 at Broad...]] for the human cell images in this study). Cluster computing prevents this from becoming a bottleneck."
[[TODO Anne (after Ray adjusts the text): put this in the methods section at the end of Software.]]

We have also added the following to the main text: "While highly dependent on the complexity of the phenotype and the scarcity of positive example cells, the entire process of training Classifier for a phenotype typically takes a few hours; of that, the hands-on time is typically less than an hour."


COMMENT 2D.2: The authors have bypassed any description of cellAnalyst, in favor of delivering a separate publication on this, but simple statements like support for commercial file formats, necessary configuration, etc. would be most valuable.  Many HCS platforms have a mix of data acquisition systems-which can be used?

Response:
Additional information in the main text would indeed be helpful. We added: "CellProfiler and CellProfiler Analyst are compatible with images acquired on most commercial microscopes, including high-throughput automated microscopes (e.g., Cellomics, Molecular Devices, Zeiss). Configuring CellProfiler Analyst to access a new CellProfiler-generated database of measurements involves minor adjustments to a properties file (CellProfiler Analyst, www.cellprofiler.org, TR Jones, IH Kang, DB Wheeler, RA Lindquist, A Papallo, DM Sabatini, P Golland, AE Carpenter, submitted)."


COMMENT 2D.3: I will admit to a personal pet peeve, as a user with over 20 years of experience in digital imaging in the life sciences.  All software is called by its authors easy to use, and almost always easily pluggable.  Yet, in fact there are only a few examples where this is in fact the case, as judged by uptake from the community- ImageJ is probably the best example in life sciences imaging (there are many others in other scientific domains, e.g., crystallography).  The authors' assertion of an modular architecture (p. 9)  should be at least illustrated with pointers to where examples and documentation for this exist, and preferably with examples of how to add new classifiers.

Response:
We concur. It is difficult to establish metrics to substantiate claims of 'easy-to-use', but we submit that many of our collaborators currently use the tool to score large-scale screens independently, usually with minimal training.  We have recently released a new interface for Classifier intended to further improve ease-of-use. The online demo shows this new Classifier tool in use (http://www.cellprofiler.com/examples). Thus, we do indeed consider this tool to be user-friendly, although we do not make this claim in the manuscript.

Regarding modularity, we agree that demonstrated use by a broad community of researchers is strong evidence. Such evidence exists for CellProfiler image analysis software (not described in this manuscript), which has been adapted to dozens of projects by researchers unaffiliated with our group, but because CellProfiler Analyst/Classifier was released only recently, we agree that it is reasonable to omit all text at the end of the discussion describing future possibilities, including substituting novel machine learning algorithms into Classifier.

===========

Reviewer #3:
COMMENT 3A: A central argument of this manuscript is that computer vision techniques can be applied to visual cell-based screens in order to find subtle phenotypes in an objective way.  The paper describes an iterative manual process to build a set of rules by manually correcting feedback from the classifier as it is being built.  Since this process is based on the user's own perception, isn't the entire process then limited by the user's ability to discern the phenotypes as well as any bias the user may introduce?  Is there a way of building and refining the classifier based solely on experimental controls without relying on a user's visual perception?  There are several examples in the literature where human observers are unable to distinguish sub-cellular morphologies that are known to be different (for example, fluorescently labeled lysosomes and endosomes).

Response: 
We address these questions and emphasize advantages of our approach in the following new text: "Previous work has demonstrated that machine learning can be successfully trained using all cells from positive and negative control samples, even for certain phenotypes that cannot be visually distinguished by humans ([[Anne TODO REF: Murphy Lab on sub-cellular localization - endosomes vs lysosomes]]). While this approach can be successful for highly penetrant phenotypes ([[suppl fig . Comparison part a]], (called Supplementary Figure 2 in the original manuscript)), it requires known positive controls and is not suitable when the phenotype is less penetrant ([[suppl fig . Comparison part b-d]]).  In our work, we address these more challenging situations and enable screens for low-penetrance phenotypes that lack positive control samples. Even when positive control samples are available, leveraging the user's visual perception to select individual example cells helps prevent the machine learning algorithm from focusing on aspects of morphology that are irrelevant to the biological question at hand, or from becoming tuned to cells that display the same complex combination of phenotypes as the positive control samples rather than the specific phenotype of interest.

[[Anne TODO: where to mention pleiotropic effects?]]
[[Anne TODO: need to decide where to put these two paragraphs in the revised manuscript; probably discussion section.]]


COMMENT 3B: This is not the first time that pattern recognition was used to evaluate a set of diverse cellular phenotypes.  This was demonstrated a decade ago by Robert Murphy's group for determining sub-cellular locations (Cytometry 1998).  Although on the face of it this addresses a different biological problem (identification of sub-cellular compartment), the visual analysis problem is the same because the classifier doesn't "know" its identifying sub-cellular locations vs. RNAi phenotypes - in both cases the problem is to identify a set of diverse cellular morphologies.  Since Murphy's work, others have built single classifiers for solving a diversity of biological and cellular imaging problems (e.g., Neumann et al., Bakal et al., Orlov et al., Chen & Murphy, as well as Loo et al. in a recent Nature Methods ).  In each of these approaches, classifiers are built using an automated process where the underlying image features are not problem-specific, making these approaches applicable to diverse imaging problems.  In light of this, the statement "none has addressed the problem of scale in terms of ability to score cells for many unusual phenotypes of interest" is not correct.  

Response:
Agreed. We should have qualified the statement, and the new version reflects this: "Thus, while recent software has solved the problem of scale in terms of ability to analyze hundreds of thousands of samples for particular phenotypes, none has addressed the problem of scale in terms of ability to score cells for many unusual phenotypes of interest when positive control samples are not available and when the phenotype is rare in the wild-type population." Please see also adjustments to the text in response to comments 1A, 1B, and 2B.


COMMENT 3C: The novelty in the proposed approach is the focus on low-penetrance phenotypes, the motivation to make this software easy to use for biologists as well as allowing for iterative/interactive manual classifier building and refinement.

Response:
Yes, and we have adjusted the text to more clearly convey these advancements (in particular, see responses to comment 1A, 1B, 2B and 3H).


COMMENT 3D: A more detailed description of the image features used is necessary.  While it may be beyond the scope of this journal to describe these algorithms in detail, the descriptions provided for what is being measured about each cell are inadequate to evaluate how the software operates.  This is equally true of the classifier algorithm itself.  GentleBoosting is  a method of combining several weak classifiers or a method of weighting a collection of image features.  It is not a classification algorithm on its own.  The actual algorithm(s) used for classification is not mentioned.  

Response:
We regret the omission of this information. We have added the list of features measured (Supplementary Data [[Anne TODO: is in ResubmissionToNatureMethods/FeaturesList.txt, needs integrating.]]), a description of the image analysis pipeline used to process the images (Supplementary Data [[Anne TODO: write the description XXX]]), and the actual image analysis pipeline that can exactly recreate the analysis in CellProfiler (Supplementary Data [[Ray TODO: see which PIPE was actually used, then give to Anne to update to current version of CP XXX]]).  This information points to the identity of the algorithms used, which are documented in CellProfiler's manual and source code.  We have clarified the main text about the classifier algorithm: "... GentleBoosting applied to regression stumps", and added a note in the methods about the specific implementation: "..., based on code from Torralba, et al." 

We have also elaborated on the application of the rules in the Methods section as follows:
"Fifty individual regression stumps constitute the final rule for each phenotype, using GentleBoosting as described in the text. For example, ÒIF(Cells_Intensity_Actin_StdIntensity > 0.076096, 0.332262, -0.606784)Ó can be translated as follows: Òif an individual cell has a standard deviation of actin pixel intensities within the whole cell greater than 0.076096, add 0.332262 to its score; otherwise subtract 0.606784 from its scoreÓ. After all 50 stumps were applied in this manner, the cell was classified as positive for the phenotype if its score was greater than zero and negative if its score was less than zero."

[[Anne TODO: put the reference to all this new supplementary material near the mention of "standard cytoprofiles".]]


COMMENT 3E: The description of the classifier evaluation is also inadequate, but extremely important for this type of publication.  How many individual images were evaluated by both the classifier and manually?  How were the test images presented to manual scorers chosen - were they selected by the classifier?  It is not possible to evaluate the false positive and false negative rates presented without a more thorough description of the evaluation procedure.  

Response:
Figure 2 and Figure 3 ("Validation" column) show the number of images that were scored by human scorers for each phenotype (e.g., 30 positive samples and 30 neutral samples for the Actin Blebs phenotype). Overall, 720 samples (360 positives, 360 neutrals) were scored by at least two humans (in addition to the computer). This, and the other questions raised by the reviewer, are now addressed more fully in the "Validation and comparison to previous methods" section: 

"We tested ClassifierÕs accuracy at ranking samples by having researchers score samples by eye. The results for Actin Blebs are shown in detail in Figure 2, and data for all phenotypes are shown in the Validation column in Figure 3. For each phenotype, Classifier rank-ordered the 5,000 puromycin-treated samples by Enrichment Score (Figure 2a), as would be done in a typical screen. For validation, researchers were forced to choose between pairs of samples: one sample in each pair had been scored by Classifier as positive (Enrichment Score greater than or equal 3) and the other as neutral (Enrichment Score roughly 0). The bar chart in Figure 2c indicates the number of times each sample was chosen as positive in these forced choices. In this example, samples that Classifier scored as positives (left, Figure 2c) were also chosen by the researchers as positives (11 or 12 times, out of 12 comparisons total), and none of the neutral samples (right, Figure 2c) was routinely chosen as positive (0 or 1 time out of 12 comparisons)."


COMMENT 3F: Lastly, it would be informative to compare the performance of this classifier to other existing classification methods.  What are the limitations of this technique? How many different phenotypes can be handled in the same experiment? How does the performance degrade when the number of rules increases? What types of phenotypes are undetectable by  Classify? 

Response:
For part of the answer to this query, please see our response above to Reviewer 2's comment 2A (Discussion of different classifier implementations). The 14 phenotypes we successfully screened in this study represent nearly every interesting, unusual phenotype we encountered in the images. Two exceptions are described in the manuscript, one of which (Peas in a Pod) was rescued by repeating image processing to measure an additional feature, and one of which (Sparkly Actin) was abandoned as described. Performance vs. number of rules is addressed by the new Supplementary Figure [[XXX]] described in our response to the next comment.


COMMENT 3G: A very real danger in applying pattern recognition techniques to visual assays is over-training.  In this application, this would manifest as an ability to robustly discern phenotypes within a set of images, but an inability to "generalize" this even to a repetition of the same experiment on a different day.  The performance of this classifier when applied to biological replicates should be looked into and discussed.

Response:
We performed additional experiments and the revised manuscript now includes the following to describe their results: "Boosting algorithms are generally resistant to overfitting [[Anne TODO - REF: Friedman...Tibshirani]]. Cross-validation results (Suppl. Figure [[XXX-cross validation results]]) show that this is also the case for our approach: the classification accuracy is not significantly reduced as the number of individual regression stumps increases. In order to increase the coverage of the training set and guard against training to a too-narrow definition of a phenotype, it is useful to inspect image of the top-ranked samples (or positive control samples, if available) where the whole field of view is visible and the positively-classified cells are marked. From these images, it is easy to identify false negative cells and add them to the training set.

None of these safeguards addresses whether a classifier will generalize to new experiments: classifiers trained on one experiment are unlikely to be applicable to experiments involving different assay protocols, cellular stains, or image acquisition instrumentation. In such cases, a new training set must be created from scratch. Using our approach, the time required to do so is minimal relative to the effort involved in sample preparation and imaging for a large-scale screen. For replicate experiments, training a classifier on one replicate and applying it to another risks negatively impacting its accuracy due to undetected experimental variation. A more robust approach is to create a training set spanning all replicates ([[Suppl. Figure YYY, Add experiments with training sets from Eggert/Castoreno (possibly also Fly200)]]). 

[[Ray TODO: Create Suppl. Figure XXX - cross validation results: error rate vs. number of rules. Do not show sens/spec, just combined error rate or xvalidation margin. Legend should explain briefly that per-cell and per-image error rates are related, but in a way we don't necessarily understand.

There are sens/spec figures in /imaging/analysis/PublicationDrafts/2008_ClassifyPaper/ResubmissionToNatureMethods/

These are for Adam C's data sets, two batches (AC34 and AC35) of multiple plates, scored for a binucleate phenotype.  They should probably show overall error rate, or classifier margin, or optimization function from GentleBoosting, rather than sens/spec.

AC34.xvalid_50rules.pdf - xvalidation on AC34
AC35.xvalid_50rules.pdf - xvalidation on AC35
Combined_xvalid_AC35_AC34.pdf - xvalidation on AC34+AC35
Crossvalid_AC35_AC34.pdf - xvalidation of AC34's classifier applied to AC35 and vice versa
Randomized_xvalid_AC35_AC34.pdf - xvalidation of AC34+AC35 with randomized labels
]]
[[Anne TODO: put new paragraphs in the 'validation' section of Results.]]


COMMENT 3H: A central criticism of this work is that the underlying basis is an image classification technique, but the paper focuses almost exclusively on its implementation in a software tool.  This is somewhat justified in that the software tool is indeed unique:  Despite previous work by others in this specific application of pattern recognition and image classification, no one has focused as much effort on making their software useable by bench scientists.  Whether this is sufficiently novel for publication in Nature Methods is an editorial decision.

Response:
Indeed, it was important to us to produce useable software to enable the approach we describe, rather than just a proof of principle. We also hope that the revised manuscript more clearly emphasizes, in response to the other reviewers' comments, that this is the first demonstration of the iterative machine learning approach to score multiple challenging and rare phenotypes in large-scale screens. 


COMMENT 3I: Additional detailed comments:
COMMENT 3I.1: Page 5. It is mentioned that 611 measurements are used without mention of what these are or even what types of measurements are involved. Some analysis could also be useful, but at the very least users should be able to know what is being measured and how. I couldn't find them in the enclosed additional papers, and also not in the CellProfiler paper published in "Genome Biology". The image analysis is one of the most important contributions of this work, and it must be described. Related to the previous comment. In page 4 ("Results"), several different characteristic are measured as a first step of the analysis. However, except from specifying that these include size, shape, intensity and textures, no information describing these measurements is provided. For instance, what characteristics of the shapes are used? How are they measured? The reference (27) does not lead to a source that specifies that. Intensity can be measured by very many different ways. Is it measured by using the statistical properties of the pixel intensity distribution? If so, which statistical properties are used? How are they measured? The term "texture" is also a very broad definition. Many techniques for measuring textures have been proposed, and the paper must describe which methods are used and how, or at least include references in which the exact same fashion of the use of these textures is specified. The rule files also feature some other measurements such as edges and polynomial decomposition. These should also be described in the text...RNAi screens for diverse phenotypes using Classifier: What are the "standard cytoprofiles"? Is there a list of the cell descriptors?

Response:
The revised manuscript now addresses these questions - please see our response to Reviewer 2's comment 2A (c: Examination of feature space explored by different phenotypes) and also our response to Reviewer 3's comment 3D. As well, we moved reference 27 because it refers to the coining of the term "cytoprofile", not information about specific measurements.


COMMENT 3I.2: Another major point is the rules. It is clear that the rules are determined by the researcher in an interactive fashion using trial and error until she is satisfied with the results.  These rules, as provided in supplementary materials need further explanation.  The rule files seem to be a set of "if" conditions. However, the manuscript does not describe how the if clauses are handled, and it is not clear how the inference computation is performed and how the ranking is determined.  It is also not clear how important the manual correction of the rules is, or if the rules can be used without manual correction. What is the contribution of the manual correction (in terms of classification accuracy) to the automatically generated rules. This point is important because the readers might be curious to know if the method can be fully automated.

Response:
The revised manuscript now addresses these questions - please see our response to comment 3D. Regarding whether the method can be fully automated, our response to comment 3A above now addresses the fact that while some screens can indeed be fully automated (and the Classifier function of CellProfiler Analyst for the first time readily enables this kind of analysis for researchers without programming skills), the main purpose of the interactive approach we present is for cases when such analysis fails.


COMMENT 3I.3: Just by looking at the rule files, it seems that some of the image content descriptors that are used are computationally intensive. A few words about the computational resources that are required and the response time of the system might be useful for researchers who consider using this method, especially on the very large datasets (10s - 100s of thousands of images) one would encounter in a screen.

Response:
The revised manuscript now addresses these questions - please see our response to Reviewer 2's comment 2D.1. 


COMMENT 3I.4: Page 10:  "50 individual rules" ==> "Fifty individual rules". (A sentence should not start with a number). Illustrations: All graphs must have clear units on both axes.

Response:
Done. We added units and labels to the cell cycle supplementary data, which appeared to be the offending figure.
[[Chris TODO]]


COMMENT 3I.5: It is totally unclear how gentleboosting is used. Boosting requires ground truth and an existing classifier (as opposed to filtering, which requires only ground truth). Since the boosting is applied here as a first step of defining the classifier rules, it is unclear what classifier is used by gentleboosting (decision trees?). It should also be mentioned that the classifier used for boosting is different than the one used for the actual classification, and therefore features that are selected might not be informative for the actual classifier. Under these circumstances, it is unclear why the authors chose to use slow boosting over much faster and classifier-independent filtering.  The number of terms used by gentleboosting is also unclear. Is this number fixed or is it determined dynamically in some way? While gentleboosting assigns weights to the features, it is unclear how the tentative rule is generated.  Also, boosting is usually a computationally intensive task. A note about this issue is also required.

Response:
We have clarified which weak learner is used (regression stumps) in the GentleBoosting algorithm. Please see our response to comment 3D, which also provides more detail about the rule selection via the Torralba, et al., reference. The performance of training and scoring via the classifier is addressed in our response to Reviewer 2's comment 2D.1.


COMMENT 3I.6: Page 5: "Few dozens" is a rough estimation.  How does a researcher determine the number? Usually, the performance of machine learning algorithms can be improved by using more samples for training. This trivial practice of machine learning might not be known to many biologists. To help the readers effectively use the described tool, this must be further discussed, preferably by measuring the effect of several different training-set sizes on classifier performance.

Response:
While the manual provides practical tips like this, we agree this issue is important enough to mention in the manuscript. The revised manuscript now addresses these questions as follows (after the sentence, "After a few dozen cells are scored, the researcher can begin the iterative machine learning phase, where the computer generates a tentative rule based on the classified cells and presents the researcher with cells classified according to that rule"):

"While more is always better when considering how many cells to classify before beginning the iterative machine learning phase (if too few are used to initiate machine learning, the computer may train itself to a too-narrow definition of the phenotype), generating a large training set in the initial stage can be difficult when the phenotype is rare or no positive control samples are available; in fact, these are the cases where the iterative nature of Classifier is necessary. The optimal initial training set depends on the complexity of the phenotype and the scarcity of positive cells in the experiment, but a good rule of thumb is to begin with at least 50 [[or, perhaps this is where we reference a new Suppl figure - to address the last sentence of the comment, it'd be super easy to make a Suppl figure showing how a phenotype (e.g. Crescents, because it's got a large training set?) responds to increased training set size by choosing random training sets and measuring cross validation accuracy on the entire training set. That'd be kind of neat and I'm curious to see the results myself.]]"

[[TODO Ray: make the plot described.]]
[[TODO Ray: wordsmithing, for length after the plot is made]]


COMMENT 3I.7: The term "scoring" is used throughout the manuscript in the sense of associating a specific sample with one of a discrete set of classes. This, however, is actually "classification", while the term "scoring" implies on some continuos indicator, e.g., Z-scores, Fisher scores, etc.

Response:
We have corrected this. Now, "classify" means to label a cell as positive or negative based on some learned rules, and "score" means to assign a continuous Enrichment Score to a particular sample. 


COMMENT 3I.8: Discussion: Since only a subset of the dataset (reference 12) was used, the reader might wonder what was the barrier for repeating the same experiment for much larger network than the 14 phenotypes. What were the criteria for choosing these phenotypes.

Response:
Please see our response to comment 3F regarding the phenotypes chosen. Also, the revised manuscript now elaborates on why we used only puromycin-treated samples for the validation step: "We used the samples treated with puromycin (the selection agent for the shRNA vector) for the validation step shown in Figure 3 because puromycin selection culls cells where the shRNA vector failed to infect, leading to more homogeneous populations in each sample, and also because puromycin affects phenotype penetrance in the wild type population."


COMMENT 3I.9: Page 9: The statement according which Classifier can be used on full organisms such as zebrafish and C. elegans is not supported by any empirical or theorethical evidence. The authors can use some of the publicly available datasets and report on the detection rate. Unless this is done, such a statement should be left out of the manuscript.

Response:
We have omitted all text at the end of the discussion describing future possibilities, including application of the approach to whole organisms.


COMMENT 3I.10: Validation and comparison to previous methods: It is clear that Classifier was compared to human detection, but the way the human detection was performed is not clear. Was each sample classified by a single researcher? What is the error rate of this manual classification? How many samples were used for training the classifier? What efforts (human resources) were required to train the classifier.

Response:
These questions have been addressed - see response to comments 2A (confusion matrices show the consistency between humans for manual classification), comment 2D.1 (describes time requirements), comment 3E (describes the evaluation procedure), and the manuscript's Figure 3 (shows the number of positive and negative example cells for each phenotype).


COMMENT 3I.11: The motivation for the forced choice test is unclear. Is there a specific reason for using this kind of test, rather than simply counting the number of samples that their automated score was in agreement with the manual score?

Response:
We have added explanation to the text: "The biologically relevant score for a sample is enrichment of cells that display the phenotype, rather than a hard 'positive' or 'negative' label. Typically, samples in screens do not fall into clear positive and negative classes but instead fall along a continuum (REF: perrimon/friedman article that talks about screens yielding smooth distributions rather than sharply defined positives). Our goal is to bring highly enriched samples to the attention of the researcher, so the validation aims to test whether top-ranking samples are indeed enriched relative to samples scored as neutral. The forced-choice comparison avoids the complication that different researchers will choose different thresholds at which they call a sample positive or negative."


COMMENT 3I.12: Also, a new descriptor (the angle between the two nearest neighbors) was added. It is not clear, however, if and how the user can add customized descriptors.

Response:
We have added this information to the main text: "Scoring a phenotype might require extracting more features from cells during the image processing step, which can be accomplished by modifying the source code of CellProfiler or by adding additional segmentation and feature extraction modules to the image analysis pipeline in CellProfiler using its graphical user interface."


COMMENT 3I.13: Page 3: "Machine learning methods that select and combine multiple features for automated cell scoring have been explored and used for a few particular phenotypes"  The cited papers do not support the statement. Previous efforts (e.g., Neumann et al., Bakal et al., Orlov et al., Chen & Murphy) have resulted in general phenotype recognition tools that can handle a large number of phenotypes, rather than focusing on detection of a few particular phenotypes. This is an important point, and it should be clear to the reader that this work is not the first attempt of general automated phenotype recognition.

Response:
These questions have now been addressed - see response to Reviewer 1's comment 1A, 1B, 2B, and 3H.

---
[[Anne TODO in final read-through: Per Bjorn's suggestion, make sure claims of novelty have been properly qualified]]
[[Anne TODO in final read-through: Check for use of 'rule' vs. 'classifier']]