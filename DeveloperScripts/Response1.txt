Response to Reviewers' Comments, and guide to changes in the manuscript:  

Reviewer #1:

Summary
The authors describe a supervised machine learning approach to score phenotypes in high-throughput image-based screens. The approach requires human experts to provide examples of cells with and without phenotypes of interest. A machine learning algorithm is then trained to recognize the provided cells, and opinions from human experts are used iteratively to improve the classification accuracy.

Major Comments
COMMENT 1A: There is an extensive prior works in the area of automated high throughput cellular phenotype profiling using microscopy images. The statement that (past) "automated image analysis requires extensive customization to each phenotype ..." is not true. For example, unbiased sets of measurements that are not customized for any particular phenotypes were used in Boland, et al., Bioinformatics (2001), Tanaka, et. al., PLOS Biology (2005), and Loo, et al., Nature Methods (2007).

Response:
Agreed, our meaning was unclear in that sentence. We replaced the sentence with: "Automated image analysis coupled with machine learning can effectively score many phenotypes but can require significant effort to adapt to rare or subtle morphologies, particularly when positive control samples are not available."

[[WAITING FOR RAY' assessment - VL: You should be explicit about in what ways the papers mentioned by the reviewer require significant effort. AC: perhaps add to the end something like, "... because, in such cases, it is often difficult to find a sufficient number of positive example cells to train the machine learning algorithm". Ray, what do you think?]]


COMMENT 1B: Further more, the authors' claim that "none has been implemented in a system proven capable of scoring millions of cells for many diverse morphologies - especially for phenotypes that are complex or subtle, without known positive controls, or present in only a small percentage of the cells in a sample". However, it appear that, as many other prior works, the proposed method also requires positive samples provided from human experts. Thus, it is unclear what do the authors mean by "without known positive controls"?

Response:
The distinction between positive control samples and positive control cells was not clear in our original sentence. To clarify, we have replaced the sentence with: "These methods require providing examples of cells that do and do not display the morphology of interest (i.e., positive and negative cells). However, finding a sufficient number of positive cells can be prohibitively difficult when positive control samples are not available and when the phenotype is rare in the wild-type population."


COMMENT 1C: The idea of using human experts to guide classifier is neither novel nor interesting. The proposed approach of identifying positive control cells from "cells noticed by chance while browsing images from the screen, or outlier cells discovered by interactively exploring cell measurements" is highly biased, non-systematic, and, worst of all, probabilistic (i.e., may not be reproducible). Although the proposed method is good at "zooming" into the phenotypes selected by human experts, it is unclear how the proposed system may discover novel cells or phenotypes that are missed by the human experts.

Response:
The goal of our work is phenotype scoring, not discovery of novel phenotypes. The sentence mentioned above describes how a researcher might choose an interesting phenotype that they wish to pursue; historically, selecting phenotypes for study has indeed been highly biased (i.e., guided by the intuition of a biologist), particularly in classic screens that have been the basis of much of modern biology. Methods exist for automated phenotype discovery, but this is not addressed by our work. We have revised the sentence to clarify: "These can be cells from control samples if the screen has been designed to address a particular phenotype, or cells identified by chance if the screen's goal is to uncover previously uncharacterized phenotypes, as in many classic screens."


COMMENT 1D: This manuscript lacks methodological novelty or biological discovery to make this appropriate for Nature Methods.

Response:
Please see our response to Reviewer 3, comment 3H.

===========

Reviewer #2:

This ms reports a new software tool, Classifier, that the authors have implemented within their existing CellAnalyst package.  Classifier is aptly named-it is a tool that provides a nice, apparently easy-to-use interface for running sophisticated feature-based supervised learning.  

There are a few successes in this field, but none to my knowledge that are routinely applied, distributed and available.  The authors should be commended for their efforts and commitment to developing useful and available software.  To my knowledge, this is unique in the image classification domain, and an important contribution for the field.  

COMMENT 2A: However, does this very useful implementation constitute a significant methodological or conceptual advance?  This is a surprisingly light treatment of an important topic that the authors properly and critically introduce-the use of automatic, supervised learning methods in large-scale cell-based screens.  As such the authors don't reach the standard achieved by other published papers in this field- reports of confusion matrices (as opposed to the very simple right/wrong analysis provided here), discussion of different classifier implementations, examination of feature space explored by different phenotypes. The authors' statement that "...automated scoring of unusual phenotypes in general can now be accomplished, if accurate cytoprofiles can be generated for the cell images" doesn't seem novel.  Admittedly, many other papers from Eils, Murphy, Ellenberg & Pepperkok, and many others referenced by the authors have focused on automatic identification of human-recognized phenotypes, but Murphy's group have already established good methods for  distinguishing phenotypes based on molecular identity that were not known to be distinguishable by humans-in short classifiers can find new phenotypes.  Against this background, the authors' contention of a significant conceptual advance seems dubious. 

Response:
The referee points out particular explorations that would be helpful to evaluate the methods we have described. We left these out of the original manuscript to maintain sharp focus, but they are now included in the revised version:
(a) Confusion matrices (Supplementary Figure [[XXX - confusion matrix]]) support the validation results originally shown in Figure 3 ("Validation" column).  In addition to the existing statement that there were no false negatives and, in total, two false positives across the 720 samples scored by eye, we have added this sentence: "Agreement between humans was comparable to that between humans and automated scoring (Supplementary Figure [[XXX - confusion matrix]])."
[[Anne TODO: get confusion matrix from Chris finalized and adjust reference in the text.]]

(b) Discussion of different classifier implementations:
Evaluation of various classifier algorithms has indeed been heavily explored in others' work, both for cell screens in the Murphy lab, as well as for many other domains in the general machine learning community. One conclusion from this work is that the top classification methods (e.g., SVMs, neural networks, boosting variants) perform roughly similarly.  Contributing to the work comparing particular learning algorithms is not our goal; rather, we present a method for rapidly creating useful training sets for rare phenotypes, using an arbitrary machine learning algorithm. In fact, we believe that any of these methods would perform well within our framework - our initial testing indicated that SVMs performed similarly to GentleBoosting on regression stumps but we chose the latter because SVMs were less suitable for generating the database queries necessary for scoring entire screens. To clarify this, we have added a sentence: "Other machine learning methods are likely to be equally effective, based on their performance in previous work [[Anne TODO: (Murphy lab, mitocheck, altschuler, bakal refs XXX)]]."

(c) Examination of feature space explored by different phenotypes:
We have created a Supplementary Figure [[XXX]] to facilitate examination of the feature space explored by each phenotype.
[[Ray TO DO: Make this figure, either lumping together into ~6 categories, or a skinny bar chart of all 611 features. Reference this figure in the text where we mention features, near 'standard cytoprofiles' discussion.]]  

(d) The referee is concerned that our statement "...automated scoring of unusual phenotypes in general can now be accomplished, if accurate cytoprofiles can be generated for the cell images" doesn't seem novel.  
We agree that the original sentence unintentionally implied that we applied machine learning to cell screening for the first time. We have adjusted the sentence to appropriately qualify the statement: "automated scoring of a wide variety of morphologies can now be accomplished quickly and easily, even when the phenotype is rare in the wild-type population and positive control samples are not available."

COMMENT 2B: The authors are first-class scientists, therefore it's worth posing a hypothetical question--  if I submitted a paper to Nature Methods reporting a new method based on an application of that method to a single dataset, would they consider that a reasonable demonstration of the general utility of the method?  This report describes the use of Classifier on one cell-based screen.  That implementation is certainly an achievement, but the authors (and editors of this journal, who have been guilty of this!) must consider the impact of reporting results based on a single run, in the absence of any significant finding derived from this analysis.  This does not diminish the work the authors have done, but recognizes the importance of delivering tested, generally useful methodology to the community.  The use of Classifier to define phenotypes across multiple screens, with multiple probes is therefore most important. Even surveying the types of assays most often done in HCS, and critically evaluating the phenotypes that are well detected and those that are less well-detected would be sufficient.  The authors must have access to this data.

Response:
The reviewer makes an excellent point. Indeed, several other projects using Classifier have been successful and are not yet published. We have now included analysis of a second image set in the manuscript. This is a different screen with different stains by different investigators, and is described more fully in our response to Reviewer 3's comment 3G about over-training and how to score replicates performed on different days (please see that response below). We should also elaborate on the screens mentioned only briefly in the text: we have assisted in the completion of several large-scale screens that probe phenotypes distinct from each other and from those explored in our own screens described in this manuscript. They were conducted by different investigators, in different cell types/organisms, using different image acquisition devices at different magnifications. These screens are, of course, intended for separate publication so we have not added detailed description to this manuscript but instead elaborate on a few of them here:

[[Gupta.tif to be pasted here (currently on imaging/analysis)]]
(a) Piyush Gupta, et al., in Eric Lander's lab have used our approach/software to complete an RNAi screen for regulators of the subtle cytoskeletal changes induced by a growth factor (HRG-b1) on T47D human breast cancer cells.

[[Mitchison.psd to be pasted here (currently on imaging/analysis)]]
(b) Taio Xie, et al., in Tim Mitchison's lab have used our approach/software to complete two genome-wide RNAi screens in HeLa cells related to spindle morphology during cell division, in enhancer/suppressor screens using two levels of a Kinesin-V inhibitor. The system was trained to identify monopolar mitotic cells.

[[Eggert.psd to be pasted here (currently on imaging/analysis)]]
(c) Adam Castereno, et al., in Riki Eggert's lab have processed a substantial proportion of images using our approach/software for a chemical screen in Drosophila Kc167 cells to identify modulators of a binucleate phenotype induced by RNA interference against Rho. 


COMMENT 2C: Moreover, a few practical but very important considerations are missing:
COMMENT 2C.1: No report on the computational performance of Classifier is mentioned.  How long does training take?  How does this scale with number of images, channels, image size, etc.?  What kinds of hardware are used for this calculation.  The authors have a very impressive compute facility at their disposal-is this required? Perhaps the application is limited by I/O, given its emphasis on large numbers of images.  Reporting these limitations is absolutely essential. 

Response:
We have added: "Computing a classifier, given a training set, is fast (for example, 25 seconds to generate a 50-rule classifier for the 1711-example Crescent Nuclei training set on a 2.33 GHz Intel Core 2 Duo), and grows linearly with training set size and the number of features. Evaluation of the classifier to classify all [[Ray TODO: 8 million cells?]] in the database takes roughly 2 minutes, with the same orders of growth. This evaluation is primarily limited by disk transfer speed, as the full data set must be read in order to score every cell. Image processing times to identify and measure cells in CellProfiler are currently on the order of 30 seconds to several minutes per image, depending on the particular experimental and image analysis used (for example, NN seconds per 512x512 [[(Ray TODO:size correct? What's the time for NIRHT?)]] three-channel image on a [[Ray TODO: what speed computer here?]] for the human images in this study). Cluster computing prevents this from becoming a bottleneck."
[[TODO Anne (after Ray adjusts the text): put this in the methods somewhere.]]


COMMENT 2C.2: The authors have bypassed any description of cellAnalyst, in favor of delivering a separate publication on this, but simple statements like support for commercial file formats, necessary configuration, etc. would be most valuable.  Many HCS platforms have a mix of data acquisition systems-which can be used?

Response:
Additional information in the main text would indeed be helpful. We added: "CellProfiler and CellProfiler Analyst are compatible with images acquired on most commercial microscopes, including high-throughput automated microscopes (e.g., Cellomics, Molecular Devices, Zeiss). Configuring CellProfiler Analyst to access a new CellProfiler-generated database of measurements involves minor adjustments to a properties file (CellProfiler Analyst, www.cellprofiler.org, TR Jones, IH Kang, DB Wheeler, RA Lindquist, A Papallo, DM Sabatini, P Golland, AE Carpenter, submitted)."


COMMENT 2C.3: I will admit to a personal pet peeve, as a user with over 20 years of experience in digital imaging in the life sciences.  All software is called by its authors easy to use, and almost always easily pluggable.  Yet, in fact there are only a few examples where this is in fact the case, as judged by uptake from the community- ImageJ is probably the best example in life sciences imaging (there are many others in other scientific domains, e.g., crystallography).  The authors' assertion of an modular architecture (p. 9)  should be at least illustrated with pointers to where examples and documentation for this exist, and preferably with examples of how to add new classifiers.

Response:
We concur. It is difficult to establish metrics to substantiate claims of 'easy-to-use', but we submit that many of our collaborators currently use the tool to score large-scale screens independently, usually with minimal training.  We have recently released a new interface for Classifier intended to further improve ease-of-use. The online demo shows this new Classifier tool in use (http://www.cellprofiler.com/examples). Thus, we do indeed consider this tool to be user-friendly, although we do not make this claim in the manuscript.

Regarding modularity, we agree that demonstrated use by a broad community of researchers is strong evidence. Such evidence exists for CellProfiler image analysis software (not described in this manuscript), which has been adapted to dozens of projects by researchers unaffiliated with our group, but because CellProfiler Analyst/Classifier was released only recently, we decided to omit all text at the end of the discussion describing future possibilities, including substituting novel machine learning algorithms.

===========

Reviewer #3:
COMMENT 3A: A central argument of this manuscript is that computer vision techniques can be applied to visual cell-based screens in order to find subtle phenotypes in an objective way.  The paper describes an iterative manual process to build a set of rules by manually correcting feedback from the classifier as it is being built.  Since this process is based on the user's own perception, isn't the entire process then limited by the user's ability to discern the phenotypes as well as any bias the user may introduce?  Is there a way of building and refining the classifier based solely on experimental controls without relying on a user's visual perception?  There are several examples in the literature where human observers are unable to distinguish sub-cellular morphologies that are known to be different (for example, fluorescently labeled lysosomes and endosomes).

Response: 
Yes, previous work (e.g., the work of the Murphy Lab on sub-cellular localization) has demonstrated that if positive controls are available and the phenotype is highly penetrant, computers can be trained to discriminate between phenotypes that cannot be visually distinguished by humans, by labeling all cells from positive control samples as positive examples and all cells from negative control samples as negative examples. Classifier provides a convenient interface for performing this type of analysis, by loading all cells from the controls into the training set without any interactive manual selection of cells.  In our work, we address the more challenging situation where the phenotype is less penetrant, positive controls are not available, or both, which makes generating training sets difficult. The failure of methods that do not leverage the user's visual perception to score such phenotypes (but instead label all positive control samples' cells as positives) is documented in Supplementary Figure [[XXX - bakal-like figure]] (called Supplementary Figure 2 in the original manuscript).

The researcher-guided definition of a phenotype has another advantage that we did not sufficiently emphasize in the original manuscript and is now described more fully:  "Even when positive control samples are available and a phenotype is highly penetrant, blindly applying machine learning to all cells of a positive control sample may not produce biologically meaningful results because positive control treatments often have pleiotropic effects on cells. In such cases, researcher-guided definition and refinement of the particular phenotype of interest using an iterative approach may be necessary in order to prevent the machine learning algorithms from focusing on certain aspects of the positive control cells' morphology that are irrelevant to the biological question at hand, or from becoming tuned to cells that display the same complex combination of phenotypes as the positive control samples rather than the specific "sub-phenotype" of interest. For example, positive control treatments often reduce cell density in addition to inducing a particular morphology of interest. Researcher-guided machine learning might thus be necessary to properly hone in on the biologically relevant morphology."

[[Ray TODO: can you edit the above paragraph? I could barely get the idea on paper, much less make it readable.]]
[[Anne TODO: need to decide where to put clarification of all this in the revised manuscript; will probably also turn the first paragraph into a real paragraph for the manuscript.]]


COMMENT 3B: This is not the first time that pattern recognition was used to evaluate a set of diverse cellular phenotypes.  This was demonstrated a decade ago by Robert Murphy's group for determining sub-cellular locations (Cytometry 1998).  Although on the face of it this addresses a different biological problem (identification of sub-cellular compartment), the visual analysis problem is the same because the classifier doesn't "know" its identifying sub-cellular locations vs. RNAi phenotypes - in both cases the problem is to identify a set of diverse cellular morphologies.  Since Murphy's work, others have built single classifiers for solving a diversity of biological and cellular imaging problems (e.g., Neumann et al., Bakal et al., Orlov et al., Chen & Murphy, as well as Loo et al. in a recent Nature Methods ).  In each of these approaches, classifiers are built using an automated process where the underlying image features are not problem-specific, making these approaches applicable to diverse imaging problems.  In light of this, the statement "none has addressed the problem of scale in terms of ability to score cells for many unusual phenotypes of interest" is not correct.  

Response:
[[Anne TODO: need to see if comment 1A and 1B address this. Probably need to add penetrance qualifier to the original sentence: "Thus, while recent software has solved the problem of scale in terms of ability to analyze hundreds of thousands of samples for particular phenotypes, none has addressed the problem of scale in terms of ability to score cells for many unusual phenotypes of interest."]]

COMMENT 3C: The novelty in the proposed approach is the focus on low-penetrance phenotypes, the motivation to make this software easy to use for biologists as well as allowing for iterative/interactive manual classifier building and refinement.

Response:
Yes, and we have adjusted the text to more clearly convey these advancements (in particular, see responses to comment 1A, 1B, and 3H).


COMMENT 3D: A more detailed description of the image features used  is necessary.  While it may be beyond the scope of this journal to describe these algorithms in detail, the descriptions provided for what is being measured about each cell are inadequate to evaluate how the software operates.  This is equally true of the classifier algorithm itself.  GentleBoosting is  a method of combining several weak classifiers or a method of weighting a collection of image features.  It is not a classification algorithm on its own.  The actual algorithm(s) used for classification is not mentioned.  

Response:
We regret the omission of this information. We have added the list of features measured (Supplementary Data [[XXX]]), a description of the image analysis pipeline used to process the images (Supplementary Data [[XXX]]), and the actual image analysis pipeline that can be run in CellProfiler to exactly recreate the analysis (Supplementary Data [[XXX]]).  This information points to the identity of the algorithms used, which are, in turn, documented in CellProfiler's manual and open source code.  We have clarified the main text on the classifier algorithm: "... GentleBoosting applied to regression stumps", and added a note in the methods about the specific implementation: "..., based on code from Torralba, et al." 

We have also elaborated on the application of the rules in the Methods section as follows:
"Fifty individual rules were used to create the final rule (i.e., classifier) for each phenotype, using GentleBoosting as described in the text. For example, an individual rule was: ÒIF(Cells_Intensity_Actin_StdIntensity > 0.076096, 0.332262, -0.606784)Ó, which can be translated as follows: Òif an individual cell has a standard deviation of actin pixel intensities within the whole cell greater than 0.076096, add 0.332262 to its score; otherwise subtract 0.606784 from its scoreÓ. After all 50 rules were applied in this manner, the cell was classified as positive for the phenotype if its score was greater than zero and negative if its score was less than zero."

[[Ray TODO: create all this Suppl data and specify which revision of CP was used; mention that old versions can be downloaded; put the reference to all this new supplementary material near the mention of "standard cytoprofiles"]]


COMMENT 3E: The description of the classifier evaluation is also inadequate, but extremely important for this type of publication.  How many individual images were evaluated by both the classifier and manually?  How were the test images presented to manual scorers chosen - were they selected by the classifier?  It is not possible to evaluate the false positive and false negative rates presented without a more thorough description of the evaluation procedure.  

Response:
Figure 2, and Figure 3 ("Validation" column), show the number of images that were scored by human scorers for each phenotype (e.g., 30 positive samples and 30 neutral samples for the Actin Blebs phenotype). Overall, 720 samples (360 positives, 360 neutrals) were scored by at least two humans (in addition to the computer). This, and the other questions raised, are now addressed more fully in the "Validation and comparison to previous methods" section: 

"We tested ClassifierÕs accuracy at ranking samples by having researchers score samples by eye. The results for Actin Blebs are shown in detail in Figure 2, and data for all phenotypes are shown in the Validation column in Figure 3. For each phenotype, Classifier rank-ordered the 5,000 puromycin-treated samples by Enrichment Score (Figure 2a), as would be done in a typical screen. For validation, researchers were forced to choose between pairs of samples: one sample in each pair had been scored by Classifier as positive (Enrichment Score greater than or equal 3) and the other as neutral (Enrichment Score roughly 0). The bar chart in Figure 2c indicates the number of times each sample was chosen as positive in these forced choices. Samples that Classifier scored as positives in this case (left, Figure 2c) were also chosen by the researchers as positives (11 or 12 times, out of 12 comparisons total), and none of the neutral samples (right, Figure 2c) was routinely chosen as positive (0 or 1 time out of 12 comparisons)."


COMMENT 3F: Lastly, it would be informative to compare the performance of this classifier to other existing classification methods.  What are the limitations of this technique? How many different phenotypes can be handled in the same experiment? How does the performance degrade when the number of rules increases? What types of phenotypes are undetectable by  Classify? 

Response:
For part of the answer to this query, please see our response above to Reviewer 2's comment 2A (Discussion of different classifier implementations). The 14 phenotypes we successfully screened in this study represent nearly every interesting, unusual phenotype we encountered in the images. Two exceptions are described in the manuscript, one of which (Peas in a Pod) was rescued by repeating image processing to measure an additional feature, and one of which (Sparkly Actin) was abandoned as described. Performance vs. number of rules is addressed by the new Supplementary Figure [[XXX]] described in our response to the next comment.


COMMENT 3G: A very real danger in applying pattern recognition techniques to visual assays is over-training.  In this application, this would manifest as an ability to robustly discern phenotypes within a set of images, but an inability to "generalize" this even to a repetition of the same experiment on a different day.  The performance of this classifier when applied to biological replicates should be looked into and discussed.

Response:
We add these new paragraphs describing additional experiments we now include to address these comments: 
"We found our approach resistant to over-training ['as more individual rules are added to the overall rule' -Ray, does adding that phrase clarify or confuse? I'm trying to address Vebjorn's comment that this whole paragraph was a little unclear] ([[Suppl. Figure XXX - cross validation results, see description below]]), as is generally the case for boosting algorithms ([[Ray TODO: provide reference]]). Another type of over-training can occur using supervised machine learning: especially with a small initial training set, the machine learning algorithm may hone in on a subset of a phenotype rather than the broader phenotype of interest. A strategy we use to prevent this is to rank-order samples after initial stages of machine learning. Looking at images of the top-ranked samples (or positive control samples, if available), where the whole field of view is visible and the positively-classified cells are marked, false negative example cells are added to the training set to 'broaden' the definition of the phenotype. [[VL: This paragraph is a little unclear. AC: Is the revised version better now?]]

[[Comments from VL: I don't think it makes sense to talk about two types of overfitting. The two types are fundamentally the same thing, namely constructing a too complex classifier (many rules) from a training set that is too sparse.  (I prefer "overfitting" to "overtraining," as the latter makes it seem as if you're giving too much training data.)  Here are some sentences that may help: "Boosting algorithms are generally resistant to overfitting [Ray TODO: citation needed].  Cross-validation results (Suppl. Figure XXX) show that this is also the case for our approach: the classification accuracy is not adversely affected to any significant degree by increasing the number of rules from 20 to 50.  In order to increase the coverage of the training set and guard against overfitting, it is useful to inspect image of the top-ranked samples (or positive control samples, if available) where the whole field of view is visible and the positively-classified cells are marked.  From these images, it is easy to identify false negative cells and add them to the training set, thereby increasing the support of the phenotype."]]

Still, none of these safeguards addresses whether a classifier will generalize to new experiments: classifiers trained on one experiment are unlikely to be applicable to experiments involving different assay protocols, cellular stains, or image acquisition instrumentation. In such cases, a new training set must be created from scratch. Using our approach, the time required to do so is minimal relative to the effort involved in sample preparation and imaging for a large-scale screen. For replicates, the best approach (in addition to minimizing experimental variation and performing normalization if needed) is to create the training set using cells taken from all replicates, to maximize the classifier's robustness to any remaining experimental variation ([[Suppl. Figure YYY, Add experiments with training sets from piyush or jacob&milan or Eggert/Castoreno (probably latter).]]). This is because training a classifier on one replicate and applying it to another risks negatively impacting its accuracy due to undetected experimental variation."
[[VL: can the last sentence be dropped? AC: I think yes, good idea. Ray, what do you think?]]

[[Ray TODO: Create Suppl. Figure XXX - cross validation results: error rate vs. number of rules. Do not show sens/spec, just combined error rate or xvalidation margin. Legend should explain briefly that per-cell and per-image error rates are related, but in a way we don't necessarily understand.

There are sens/spec figures in /imaging/analysis/PublicationDrafts/2008_ClassifyPaper/ResubmissionToNatureMethods/

These are for Adam C's data sets, two batches (AC34 and AC35) of multiple plates, scored for a binucleate phenotype.  They should probably show overall error rate, or classifier margin, or optimization function from GentleBoosting, rather than sens/spec.

AC34.xvalid_50rules.pdf - xvalidation on AC34
AC35.xvalid_50rules.pdf - xvalidation on AC35
Combined_xvalid_AC35_AC34.pdf - xvalidation on AC34+AC35
Crossvalid_AC35_AC34.pdf - xvalidation of AC34's classifier applied to AC35 and vice versa
Randomized_xvalid_AC35_AC34.pdf - xvalidation of AC34+AC35 with randomized labels
]]

COMMENT 3H: A central criticism of this work is that the underlying basis is an image classification technique, but the paper focuses almost exclusively on its implementation in a software tool.  This is somewhat justified in that the software tool is indeed unique:  Despite previous work by others in this specific application of pattern recognition and image classification, no one has focused as much effort on making their software useable by bench scientists.  Whether this is sufficiently novel for publication in Nature Methods is an editorial decision.

Response:
Indeed, it was important to us to produce useable software to enable the approach we describe, rather than just a proof of principle. We also hope that the revised manuscript more clearly emphasizes, in response to the other reviewers' comments, that this is the first demonstration of the iterative machine learning approach to score multiple challenging and rare phenotypes in large-scale screens. 

[[VL: This is a nice, succinct statement of the paper's contribution.  If it's not already in the abstract, it should be. AC: good idea - Anne TODO. AC: actually, in light of Bjorn's concerns of novelty claims perhaps we'd best leave this out of the abstract though leaving it in the response is probably fine, and we might put it elsewhere, softened somewhat. I guess we can't rule out that others' screens have been somewhat iterative (even if not conveniently so). Ray, what do you think?]]


COMMENT 3I: Additional detailed comments:
COMMENT 3I.1: Page 5. It is mentioned that 611 measurements are used without mention of what these are or even what types of measurements are involved. Some analysis could also be useful, but at the very least users should be able to know what is being measured and how. I couldn't find them in the enclosed additional papers, and also not in the CellProfiler paper published in "Genome Biology". The image analysis is one of the most important contributions of this work, and it must be described. Related to the previous comment. In page 4 ("Results"), several different characteristic are measured as a first step of the analysis. However, except from specifying that these include size, shape, intensity and textures, no information describing these measurements is provided. For instance, what characteristics of the shapes are used? How are they measured? The reference (27) does not lead to a source that specifies that. Intensity can be measured by very many different ways. Is it measured by using the statistical properties of the pixel intensity distribution? If so, which statistical properties are used? How are they measured? The term "texture" is also a very broad definition. Many techniques for measuring textures have been proposed, and the paper must describe which methods are used and how, or at least include references in which the exact same fashion of the use of these textures is specified. The rule files also feature some other measurements such as edges and polynomial decomposition. These should also be described in the text...RNAi screens for diverse phenotypes using Classifier: What are the "standard cytoprofiles"? Is there a list of the cell descriptors?

Response:
The revised manuscript now addresses these questions - please see our response to Reviewer 2's comment 2A (c: Examination of feature space explored by different phenotypes) and also our response to Reviewer 3's comment 3D. As well, we moved reference 27 because it refers to the coining of the term "cytoprofile", not information about specific measurements.


COMMENT 3I.2: Another major point is the rules. It is clear that the rules are determined by the researcher in an interactive fashion using trial and error until she is satisfied with the results.  These rules, as provided in supplementary materials need further explanation.  The rule files seem to be a set of "if" conditions. However, the manuscript does not describe how the if clauses are handled, and it is not clear how the inference computation is performed and how the ranking is determined.  It is also not clear how important the manual correction of the rules is, or if the rules can be used without manual correction. What is the contribution of the manual correction (in terms of classification accuracy) to the automatically generated rules. This point is important because the readers might be curious to know if the method can be fully automated.

Response:
Please see the revised manuscript's new text above, in our response to comment 3D. Regarding whether the method can be fully automated, our response to comment 3A above now addresses the fact that while some screens can indeed be fully automated (and the Classifier function of CellProfiler Analyst for the first time readily enables this kind of analysis for researchers without programming skills), the main purpose of the interactive approach we present is for cases when such analysis fails.


COMMENT 3I.3: Just by looking at the rule files, it seems that some of the image content descriptors that are used are computationally intensive. A few words about the computational resources that are required and the response time of the system might be useful for researchers who consider using this method, especially on the very large datasets (10s - 100s of thousands of images) one would encounter in a screen.

Response:
The revised manuscript now addresses these questions - please see our response to Reviewer 2's comment 2C.1. 


COMMENT 3I.4: Page 10:  "50 individual rules" ==> "Fifty individual rules". (A sentence should not start with a number). Illustrations: All graphs must have clear units on both axes.

Response:
Done.
[[Anne TO DO: need to figure out which figures are referenced and fix them. 

Ray: I think they mean every subplot.  I would push back, if so.   AC: Ah, I see. In that case, it will be easy to create the figure with every axis labeled and submit two versions for the editors to decide. Presumably, the full-labeled version will look hideous.  OH WAIT, does he mean numerical 'units' or just the text axes labels? Perhaps some of our plots don't have subdivisions at all? need to check.]]

COMMENT 3I.5: It is totally unclear how gentleboosting is used. Boosting requires ground truth and an existing classifier (as opposed to filtering, which requires only ground truth). Since the boosting is applied here as a first step of defining the classifier rules, it is unclear what classifier is used by gentleboosting (decision trees?). It should also be mentioned that the classifier used for boosting is different than the one used for the actual classification, and therefore features that are selected might not be informative for the actual classifier. Under these circumstances, it is unclear why the authors chose to use slow boosting over much faster and classifier-independent filtering.  The number of terms used by gentleboosting is also unclear. Is this number fixed or is it determined dynamically in some way? While gentleboosting assigns weights to the features, it is unclear how the tentative rule is generated.  Also, boosting is usually a computationally intensive task. A note about this issue is also required.

Response:
We have clarified which weak learner is used (regression stumps) in the GentleBoosting algorithm.  Please see our response to comment 3D.  [[Ray, is this sentence necessary? If so, it should be integrated into the response to 3D: "The boosted classifier is made up of multiple stumps, which provides some information about relevant features, though their combination is more complex than any single feature."]] The performance of training of and scoring via the classifier is addressed in our response to Reviewer 2's comment 2C.1.  More detail is given about the rule selection via the reference to Torralba, et al. (see also response to comment 3D).  

[[Ray: I'm not sure what they mean by classifier-independent filtering.   AC: Not really sure, but it seems there might be confusion that building the training set uses a fundamentally different algorithm than the classify-all-cells-in-the-screen step? I really don't know. Vebjorn: what do you think?]]


COMMENT 3I.6: Page 5: "Few dozens" is a rough estimation.  How does a researcher determine the number? Usually, the performance of machine learning algorithms can be improved by using more samples for training. This trivial practice of machine learning might not be known to many biologists. To help the readers effectively use the described tool, this must be further discussed, preferably by measuring the effect of several different training-set sizes on classifier performance.

Response:
While the manual provides practical tips like this, we agree this issue is important enough to mention in the manuscript. The revised manuscript now addresses these questions as follows (after the sentence, "After a few dozen cells are scored, the researcher can begin the iterative machine learning phase, where the computer generates a tentative rule based on the scored cells and presents the researcher with cells scored according to that rule"):

"While more is always better when considering how many cells to score before beginning the iterative machine learning phase (if too few are used to initiate machine learning, the computer may train itself to a too-narrow definition of the phenotype), generating a large training set in the initial stage can be difficult when the phenotype is rare or no positive control samples are available; in fact, these are the cases where the iterative nature of Classifier is necessary. The optimal initial training set depends on the complexity of the phenotype and the scarcity of positive cells in the experiment, but a good rule of thumb is to begin with at least 50 [[or, perhaps this is where we reference a new Suppl figure - to address the last sentence of the comment, it'd be super easy to make a Suppl figure showing how a phenotype (e.g. Crescents, because it's got a large training set?) responds to increased training set size by choosing random training sets and measuring cross validation accuracy on the entire training set. That'd be kind of neat and I'm curious to see the results myself.]]"


[[Ray: can we shorten this? AC: Sure, someone can give it a try or you and I can do it Monday. I wrote it so someone else should edit!  :) ]]
[[AC: Ray, what do you think about doing the analysis I describe above.]]


COMMENT 3I.7: The term "scoring" is used throughout the manuscript in the sense of associating a specific sample with one of a discrete set of classes. This, however, is actually "classification", while the term "scoring" implies on some continuos indicator, e.g., Z-scores, Fisher scores, etc.

Response:
We have corrected the imprecise language: now, "classify" means to label a cell as positive or negative based on some learned rules, and "score" means to assign a continuous Enrichment Score to a particular sample. 


COMMENT 3I.8: Discussion: Since only a subset of the dataset (reference 12) was used, the reader might wonder what was the barrier for repeating the same experiment for much larger network than the 14 phenotypes. What were the criteria for choosing these phenotypes.

Response:
[[Ref 12 is the Moffat paper. I'm not sure what the query is - they think we used fewer than the number of samples in the Moffat paper (which is only true in the sense of puro+ instead of all)? But then the query says 'much larger network than the 14 phenotypes'. Not sure what that means. For the phenotypes, we could refer to our response to comment 3F "The 14 phenotypes we successfully screened in this study represent nearly every interesting, unusual phenotype we encountered in the images."]]

[[Note, the original sentence is: "We used the samples treated with puromycin (the selection agent for the shRNA vector) for the validation step shown in Figure 3." and we could easily add "... because XXX (I don't remember why you decided that)."]]

[[Ray: How about: "... for two reasons: that we expect puromycin to affect phenotype penetrance in the wild type population, and because puromycin selection culls cells where the shRNA vector failed to infect, leading to more homogeneous populations in each sample."]]
[[AC: Really, 'we expect puromycin to affect phenotype penetrance in the wild type population'? Isn't it just that 'puromycin might affect phenotype penetrance in the wild type population'?  But anyway, is that really why we did this?  I actually don't remember, but the latter reasoning makes more sense than the former - or are those two statements actually just one argument?]]
[[AC: Vebjorn, can you provide additional insight as to what the reviewer might mean, and do you think the proposed response is fine?]]


COMMENT 3I.9: Page 9: The statement according which Classifier can be used on full organisms such as zebrafish and C. elegans is not supported by any empirical or theorethical evidence. The authors can use some of the publicly available datasets and report on the detection rate. Unless this is done, such a statement should be left out of the manuscript.

Response:
We have omitted all text at the end of the discussion describing future possibilities, including application of the approach to whole organisms.


COMMENT 3I.10: Validation and comparison to previous methods: It is clear that Classifier was compared to human detection, but the way the human detection was performed is not clear. Was each sample classified by a single researcher? What is the error rate of this manual classification? How many samples were used for training the classifier? What efforts (human resources) were required to train the classifier.

Response:
These questions have now been addressed - see response to comments [[Anne TO DO: X Y Z]].


COMMENT 3I.11: The motivation for the forced choice test is unclear. Is there a specific reason for using this kind of test, rather than simply counting the number of samples that their automated score was in agreement with the manual score?

Response:
We have added explanation to the text: "The biologically relevant score for a sample is enrichment of cells that display the phenotype, rather than a hard 'positive' or 'negative' label, because these samples, as in most screens (REF: perrimon/friedman article that talks about screens yielding smooth distributions rather than sharply defined positives), do not cleanly fall into clear positive and negative classes. Because our goal is to bring highly enriched samples to the attention of the researcher, our aim is to ensure that top-ranking samples are indeed enriched relative to samples scored as neutral. Scoring two samples relative to each other thus avoids the complication that different researchers will subconsciously choose different 'thresholds' at which they call a sample positive or negative."

[[Ray, do you find this convincing? Perhaps you also have a more technical response to add to this one? Feel free to adjust.

Ray: I have wordsmithing ideas, but they can wait until Monday.]]

[[VL: I think it's convincing, but the last sentence could be clearer.]]


COMMENT 3I.12: Also, a new descriptor (the angle between the two nearest neighbors) was added. It is not clear, however, if and how the user can add customized descriptors.

Response:
We have added this information to the main text: "Scoring some phenotypes might be improved by extracting more features from cells during the image processing step, which can be accomplished in two ways: (1) Additional segmentation and feature extraction modules can be added to the image analysis pipeline in CellProfiler (e.g., to define a new cellular compartment and measure its features) using its graphical user interface. (2) Additional features can be extracted by modifying the source code of CellProfiler."

[[Ray: I would like to wordsmith this on Monday.  Currently too long.  AC: Is the new version ok now? Significantly more concise, and I don't mind losing some of the content if you think it's ok.]]

COMMENT 3I.13: Page 3: "Machine learning methods that select and combine multiple features for automated cell scoring have been explored and used for a few particular phenotypes"  The cited papers do not support the statement. Previous efforts (e.g., Neumann et al., Bakal et al., Orlov et al., Chen & Murphy) have resulted in general phenotype recognition tools that can handle a large number of phenotypes, rather than focusing on detection of a few particular phenotypes. This is an important point, and it should be clear to the reader that this work is not the first attempt of general automated phenotype recognition.

Response:
These questions have now been addressed - see response to Reviewer 1's comment 1A.
[[Anne TO DO: Check whether that's correct and search this response for other places we address this (search for 'explored')]]

---
[[TODO: Per Bjorn's suggestion, make sure claims of novelty have been properly qualified]]
[[TODO: Check for use of 'rule' vs. 'classifier']]

[[TODO: cite both Torralba as well as Friedman & Tibshirani (or whatever the GB base ref is). AC: Ray, I have the Torralba reference and this is the GentleBoosting one; if it's correct you can delete this note now: 28. Friedman, J.H., Hastie, T. & Tibshirani, R. Additive logistic regression : a statistical view of boosting. Ann. Statist. 28, 337-407 (2000).]]